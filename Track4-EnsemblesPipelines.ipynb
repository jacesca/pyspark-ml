{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d9f96b6-f713-43fe-a543-164a5372f1c5",
   "metadata": {},
   "source": [
    "# PySpark ML - Ensembles & Pipelines\n",
    "\n",
    "Finally you'll learn how to make your models more efficient. You'll find out how to use pipelines to make your code clearer and easier to maintain. Then you'll use cross-validation to better test your models and select good model parameters. Finally you'll dabble in two types of ensemble model.\n",
    "\n",
    "## Preparing the environment\n",
    "\n",
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c21458f8-1acd-43a2-9cc6-306b920957ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from environment import SEED\n",
    "from pprint import pprint\n",
    "from pyspark.sql.types import (StructType, StructField,\n",
    "                               DoubleType, IntegerType, StringType)\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.mllib.linalg import DenseVector\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import (OneHotEncoder, StringIndexer, VectorAssembler,\n",
    "                                Tokenizer, StopWordsRemover, HashingTF, IDF)\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.classification import (LogisticRegression, RandomForestClassifier,\n",
    "                                       GBTClassifier, DecisionTreeClassifier)\n",
    "from pyspark.ml.evaluation import RegressionEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb12e47-14ed-4e04-bc56-4e38f659c2b3",
   "metadata": {},
   "source": [
    "### Connect to Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "067fae1d-87c8-420a-9970-2287108b60f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.1\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession.builder\n",
    "                     .master('local[*]') \\\n",
    "                     .appName('spark_application') \\\n",
    "                     .config(\"spark.sql.repl.eagerEval.enabled\", True)  # eval DataFrame in notebooks\n",
    "                     .getOrCreate())\n",
    "\n",
    "sc = spark.sparkContext\n",
    "print(f'Spark version: {spark.version}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ea8ed5-a73b-4a7f-9587-9dff5833f1d9",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "### Flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "581ac1df-5a2d-4399-badf-5650f82ca852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape: (47022, 11)\n",
      "root\n",
      " |-- mon: integer (nullable = true)\n",
      " |-- dom: integer (nullable = true)\n",
      " |-- dow: integer (nullable = true)\n",
      " |-- carrier: string (nullable = true)\n",
      " |-- flight: integer (nullable = true)\n",
      " |-- org: string (nullable = true)\n",
      " |-- mile: integer (nullable = true)\n",
      " |-- depart: double (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- delay: integer (nullable = true)\n",
      " |-- km: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>mon</th><th>dom</th><th>dow</th><th>carrier</th><th>flight</th><th>org</th><th>mile</th><th>depart</th><th>duration</th><th>delay</th><th>km</th></tr>\n",
       "<tr><td>0</td><td>22</td><td>2</td><td>UA</td><td>1107</td><td>ORD</td><td>316</td><td>16.33</td><td>82</td><td>30</td><td>509.0</td></tr>\n",
       "<tr><td>2</td><td>20</td><td>4</td><td>UA</td><td>226</td><td>SFO</td><td>337</td><td>6.17</td><td>82</td><td>-8</td><td>542.0</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+---+---+-------+------+---+----+------+--------+-----+-----+\n",
       "|mon|dom|dow|carrier|flight|org|mile|depart|duration|delay|   km|\n",
       "+---+---+---+-------+------+---+----+------+--------+-----+-----+\n",
       "|  0| 22|  2|     UA|  1107|ORD| 316| 16.33|      82|   30|509.0|\n",
       "|  2| 20|  4|     UA|   226|SFO| 337|  6.17|      82|   -8|542.0|\n",
       "+---+---+---+-------+------+---+----+------+--------+-----+-----+"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the file\n",
    "schema_flights = StructType([\n",
    "    StructField(\"mon\", IntegerType()),\n",
    "    StructField(\"dom\", IntegerType()),\n",
    "    StructField(\"dow\", IntegerType()),\n",
    "    StructField(\"carrier\", StringType()),\n",
    "    StructField(\"flight\", IntegerType()),\n",
    "    StructField(\"org\", StringType()),\n",
    "    StructField(\"mile\", IntegerType()),\n",
    "    StructField(\"depart\", DoubleType()),\n",
    "    StructField(\"duration\", IntegerType()),\n",
    "    StructField(\"delay\", IntegerType())\n",
    "])\n",
    "flights_data = spark.read.csv('data-sources/flights.csv', header=True, schema=schema_flights, nullValue='NA')\n",
    "\n",
    "# Cleaning and mutating some columns\n",
    "flights_data = flights_data.dropna()\n",
    "flights_data = flights_data.withColumn('km', F.round(flights_data['mile'] * 1.60934, 0))\n",
    "\n",
    "# Reviewing the result\n",
    "flights_data.createOrReplaceTempView(\"flights\")\n",
    "print(f'Dataframe shape: ({flights_data.count()}, {len(flights_data.columns)})')\n",
    "flights_data.printSchema()\n",
    "flights_data.limit(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a338d66b-b8a4-45ea-866b-25e271d7e3af",
   "metadata": {},
   "source": [
    "### SMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dca86e2e-43d4-4688-919a-c7740c20ae68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape: (5574, 3)\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>id</th><th>text</th><th>label</th></tr>\n",
       "<tr><td>1</td><td>Sorry, I'll call ...</td><td>0</td></tr>\n",
       "<tr><td>2</td><td>Dont worry. I gue...</td><td>0</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+--------------------+-----+\n",
       "| id|                text|label|\n",
       "+---+--------------------+-----+\n",
       "|  1|Sorry, I'll call ...|    0|\n",
       "|  2|Dont worry. I gue...|    0|\n",
       "+---+--------------------+-----+"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the file\n",
    "schema_sms = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"text\", StringType()),\n",
    "    StructField(\"label\", IntegerType())\n",
    "])\n",
    "sms_data = spark.read.csv(\"data-sources/sms.csv\", sep=';', header=False, schema=schema_sms)\n",
    "\n",
    "# Reviewing the result\n",
    "sms_data.createOrReplaceTempView(\"sms\")\n",
    "print(f'Dataframe shape: ({sms_data.count()}, {len(sms_data.columns)})')\n",
    "sms_data.printSchema()\n",
    "sms_data.limit(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc98025-d53b-480d-965d-9e24d5ebdf9c",
   "metadata": {},
   "source": [
    "### Cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cad91f26-558e-4cc4-a0c9-c02c1bfddcb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape: (92, 11)\n",
      "root\n",
      " |-- maker: string (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- cyl: integer (nullable = true)\n",
      " |-- size: double (nullable = true)\n",
      " |-- weight: integer (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- rpm: integer (nullable = true)\n",
      " |-- consumption: double (nullable = true)\n",
      " |-- mass: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>maker</th><th>model</th><th>origin</th><th>type</th><th>cyl</th><th>size</th><th>weight</th><th>length</th><th>rpm</th><th>consumption</th><th>mass</th></tr>\n",
       "<tr><td>Geo</td><td>Metro</td><td>non-USA</td><td>Small</td><td>3</td><td>1.0</td><td>1695</td><td>3.835</td><td>5700</td><td>7.571</td><td>769.0</td></tr>\n",
       "<tr><td>Honda</td><td>Civic</td><td>non-USA</td><td>Small</td><td>4</td><td>1.5</td><td>2350</td><td>4.394</td><td>5900</td><td>8.214</td><td>1066.0</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----+-----+-------+-----+---+----+------+------+----+-----------+------+\n",
       "|maker|model| origin| type|cyl|size|weight|length| rpm|consumption|  mass|\n",
       "+-----+-----+-------+-----+---+----+------+------+----+-----------+------+\n",
       "|  Geo|Metro|non-USA|Small|  3| 1.0|  1695| 3.835|5700|      7.571| 769.0|\n",
       "|Honda|Civic|non-USA|Small|  4| 1.5|  2350| 4.394|5900|      8.214|1066.0|\n",
       "+-----+-----+-------+-----+---+----+------+------+----+-----------+------+"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the file\n",
    "schema_cars = StructType([\n",
    "    StructField(\"maker\", StringType()),\n",
    "    StructField(\"model\", StringType()),\n",
    "    StructField(\"origin\", StringType()),\n",
    "    StructField(\"type\", StringType()),\n",
    "    StructField(\"cyl\", IntegerType()),\n",
    "    StructField(\"size\", DoubleType()),\n",
    "    StructField(\"weight\", IntegerType()),\n",
    "    StructField(\"length\", DoubleType()),\n",
    "    StructField(\"rpm\", IntegerType()),\n",
    "    StructField(\"consumption\", DoubleType())\n",
    "])\n",
    "cars_data = spark.read.csv('data-sources/cars.csv', header=True, schema=schema_cars, nullValue='NA')\n",
    "\n",
    "# Cleaning and mutating some columns\n",
    "cars_data = cars_data.dropna()\n",
    "cars_data = cars_data.withColumn('mass', F.round(cars_data.weight / 2.205, 0))\n",
    "cars_data = cars_data.withColumn('length', F.round(cars_data.length * 0.0254, 3))\n",
    "cars_data = cars_data.withColumn('consumption', F.round(cars_data.consumption * 3.78541, 3))\n",
    "\n",
    "# Reviewing the result\n",
    "cars_data.createOrReplaceTempView(\"cars\")\n",
    "print(f'Dataframe shape: ({cars_data.count()}, {len(cars_data.columns)})')\n",
    "cars_data.printSchema()\n",
    "cars_data.limit(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c6d660-df30-4447-b388-f6bc0c9ea2ce",
   "metadata": {},
   "source": [
    "### Books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eca1f144-9dab-434e-851f-e08d48710c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape: (92, 11)\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>id</th><th>text</th></tr>\n",
       "<tr><td>0</td><td>Forever, or a Lon...</td></tr>\n",
       "<tr><td>1</td><td>Winnie-the-Pooh</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+--------------------+\n",
       "| id|                text|\n",
       "+---+--------------------+\n",
       "|  0|Forever, or a Lon...|\n",
       "|  1|     Winnie-the-Pooh|\n",
       "+---+--------------------+"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the file\n",
    "schema_books = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"text\", StringType())\n",
    "])\n",
    "books_data = spark.read.csv(\"data-sources/books.csv\", sep=';', header=True, schema=schema_books)\n",
    "\n",
    "# Reviewing the result\n",
    "books_data.createOrReplaceTempView(\"books\")\n",
    "print(f'Dataframe shape: ({cars_data.count()}, {len(cars_data.columns)})')\n",
    "books_data.printSchema()\n",
    "books_data.limit(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb681615-16b7-4d06-9195-23e472dcb1ef",
   "metadata": {},
   "source": [
    "### BMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1c69093-463c-4eac-8990-99042b47a5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape: (92, 11)\n",
      "root\n",
      " |-- height_mt: double (nullable = true)\n",
      " |-- mass_kg: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>height_mt</th><th>mass_kg</th></tr>\n",
       "<tr><td>1.7496714153011232</td><td>54.45891245893144</td></tr>\n",
       "<tr><td>1.6861735698828815</td><td>63.856884442309884</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------------------+------------------+\n",
       "|         height_mt|           mass_kg|\n",
       "+------------------+------------------+\n",
       "|1.7496714153011232| 54.45891245893144|\n",
       "|1.6861735698828815|63.856884442309884|\n",
       "+------------------+------------------+"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the file\n",
    "schema_bmi = StructType([\n",
    "    StructField(\"height_mt\", DoubleType()),\n",
    "    StructField(\"mass_kg\", DoubleType())\n",
    "])\n",
    "bmi_data = spark.read.csv(\"data-sources/bmi.csv\", sep=',', header=True, schema=schema_bmi)\n",
    "\n",
    "# Reviewing the result\n",
    "bmi_data.createOrReplaceTempView(\"bmi\")\n",
    "print(f'Dataframe shape: ({cars_data.count()}, {len(cars_data.columns)})')\n",
    "bmi_data.printSchema()\n",
    "bmi_data.limit(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79709f1-ff5e-4b2b-8168-a273730bb43f",
   "metadata": {},
   "source": [
    "### Tables catalogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "544d8bd5-3e6f-4f7c-9f51-c4cee7a56df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='bmi', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='books', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='cars', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='flights', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='sms', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f62fb5-de20-4811-97b5-f349708f3047",
   "metadata": {},
   "source": [
    "# Cars Dataset\n",
    "\n",
    "## First Pipeline Model\n",
    "\n",
    "### Applying steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de5f29aa-b3df-4f2b-b352-852b93e7f43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 75, Testing set: 17\n"
     ]
    }
   ],
   "source": [
    "# Loading the data\n",
    "df_cars = cars_data.select('*')\n",
    "\n",
    "# setting the label and features columns\n",
    "label_col = 'consumption'\n",
    "feature_cols = ['mass', 'cyl', 'type_vec']\n",
    "\n",
    "# Setting the steps\n",
    "indexer_car = StringIndexer(inputCol='type', outputCol='type_idx')\n",
    "onehot_car = OneHotEncoder(inputCols=['type_idx'], outputCols=['type_vec'])\n",
    "assemble_car = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "regression_car = LinearRegression(labelCol=label_col)\n",
    "\n",
    "# Split into train and test set.\n",
    "df_cars_train, df_cars_test = df_cars.randomSplit([0.8, 0.2], seed=SEED)\n",
    "print(f\"Training set: {df_cars_train.count()}, Testing set: {df_cars_test.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd12a62-35db-481d-a68b-90d626d6831f",
   "metadata": {},
   "source": [
    "### Building the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62df12b0-eb3c-4f78-b7d2-4a8fd4c96b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RMSE: 0.8852987870756872\n",
      " MAE: 0.7074495911938766\n",
      "  R²: 0.8105462852852746\n",
      " MSE: 0.783753942397683\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the pipeline\n",
    "pipeline_car = Pipeline(stages=[indexer_car, onehot_car, assemble_car, regression_car])\n",
    "\n",
    "# Fitting the model on the trainning data\n",
    "pipeline_car = pipeline_car.fit(df_cars_train)\n",
    "df_cars_train = pipeline_car.transform(df_cars_train)\n",
    "\n",
    "# Evaluating on the testing data\n",
    "predictions_cars = pipeline_car.transform(df_cars_test)\n",
    "\n",
    "eval_cars = RegressionEvaluator(labelCol=label_col)\n",
    "print(f'''\n",
    "RMSE: {eval_cars.evaluate(predictions_cars)}\n",
    " MAE: {eval_cars.evaluate(predictions_cars, {eval_cars.metricName: \"mae\"})}\n",
    "  R²: {eval_cars.evaluate(predictions_cars, {eval_cars.metricName: \"r2\"})}\n",
    " MSE: {eval_cars.evaluate(predictions_cars, {eval_cars.metricName: \"mse\"})}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b87ed28-c21e-435a-b837-1941b906061f",
   "metadata": {},
   "source": [
    "### Stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9640592b-6a13-41fd-8221-5531e4d83aa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexerModel: uid=StringIndexer_055199545c4c, handleInvalid=error,\n",
       " OneHotEncoderModel: uid=OneHotEncoder_d43fadc12aaf, dropLast=true, handleInvalid=error, numInputCols=1, numOutputCols=1,\n",
       " VectorAssembler_c3e21b76706b,\n",
       " LinearRegressionModel: uid=LinearRegression_16325fae09c0, numFeatures=7]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reviewing the steps in the pipeline\n",
    "pipeline_car.stages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6ab081-d817-4359-8794-d45d1856a0fd",
   "metadata": {},
   "source": [
    "### Interpreting intercept & Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa245106-ca74-44f5-be24-0217c5a9bec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------+\n",
      "|   type|type_idx|     type_vec|\n",
      "+-------+--------+-------------+\n",
      "|Midsize|     0.0|(5,[0],[1.0])|\n",
      "|  Small|     1.0|(5,[1],[1.0])|\n",
      "|Compact|     2.0|(5,[2],[1.0])|\n",
      "| Sporty|     3.0|(5,[3],[1.0])|\n",
      "|    Van|     4.0|(5,[4],[1.0])|\n",
      "|  Large|     5.0|    (5,[],[])|\n",
      "+-------+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cars_train.select('type', 'type_idx', 'type_vec').distinct().sort('type_idx').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db9451a2-3545-4e6c-8079-b8b7aadbbaa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Intercept: 4.451124924882582\n",
      "Coefficients:\n",
      "[0.0044227611335224395, 0.37491422671277763, 0.7328547001153257, 0.4402829780657993, 0.9579155303172929, 1.1222235235549773, 3.207167823278289]\n",
      "\n",
      "Feature cols: ['mass', 'cyl', 'type_vec']\n",
      "Encoded categos (type_vec): ['Midsize', 'Small', 'Compact', 'Sporty', 'Van']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Slope</th>\n",
       "      <th>Feature</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intercept</td>\n",
       "      <td></td>\n",
       "      <td>4.451125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Coefficients</td>\n",
       "      <td>mass</td>\n",
       "      <td>0.004423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coefficients</td>\n",
       "      <td>cyl</td>\n",
       "      <td>0.374914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Coefficients</td>\n",
       "      <td>Midsize</td>\n",
       "      <td>0.732855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Coefficients</td>\n",
       "      <td>Small</td>\n",
       "      <td>0.440283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Coefficients</td>\n",
       "      <td>Compact</td>\n",
       "      <td>0.957916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Coefficients</td>\n",
       "      <td>Sporty</td>\n",
       "      <td>1.122224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Coefficients</td>\n",
       "      <td>Van</td>\n",
       "      <td>3.207168</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Slope  Feature     value\n",
       "0     Intercept           4.451125\n",
       "1  Coefficients     mass  0.004423\n",
       "2  Coefficients      cyl  0.374914\n",
       "3  Coefficients  Midsize  0.732855\n",
       "4  Coefficients    Small  0.440283\n",
       "5  Coefficients  Compact  0.957916\n",
       "6  Coefficients   Sporty  1.122224\n",
       "7  Coefficients      Van  3.207168"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the intercept, coefficients and features\n",
    "intercept = pipeline_car.stages[3].intercept\n",
    "coefficients = list(pipeline_car.stages[3].coefficients)\n",
    "\n",
    "colIdx =  sorted((value, key) for (key, value) \n",
    "                 in df_cars_train.select('type', \"type_idx\").distinct().rdd.collectAsMap().items())\n",
    "newCols_type = list(map(lambda x: x[1], colIdx))\n",
    "\n",
    "print(f'''\n",
    "Intercept: {intercept}\n",
    "Coefficients:\n",
    "{coefficients}\n",
    "\n",
    "Feature cols: {feature_cols}\n",
    "Encoded categos (type_vec): {newCols_type[:-1]}\n",
    "''')\n",
    "\n",
    "# With more detail\n",
    "coefficient_len = len(coefficients)\n",
    "pd.DataFrame({\n",
    "    'Slope': ['Intercept'] + ['Coefficients']*coefficient_len,\n",
    "    'Feature': [''] + feature_cols[:-1] + newCols_type[:-1],\n",
    "    'value': [intercept] + coefficients\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a3c8a1-e101-418b-aef2-8f73790e7626",
   "metadata": {},
   "source": [
    "## Cross-Validation\n",
    "\n",
    "### Building the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5228aa5a-27eb-4dab-930a-0e3557a95ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 75, Testing set: 17\n"
     ]
    }
   ],
   "source": [
    "# Loading the data\n",
    "df_cars = cars_data.select('*')\n",
    "\n",
    "# setting the label and features columns\n",
    "label_col = 'consumption'\n",
    "feature_cols = ['mass', 'cyl']\n",
    "\n",
    "# Setting the steps\n",
    "assemble_car = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "regression_car = LinearRegression(labelCol=label_col)\n",
    "\n",
    "# Split into train and test set.\n",
    "df_cars_train, df_cars_test = df_cars.randomSplit([0.8, 0.2], seed=SEED)\n",
    "print(f\"Training set: {df_cars_train.count()}, Testing set: {df_cars_test.count()}\")\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline_car = Pipeline(stages=[assemble_car, regression_car])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdd876d-4b3a-41df-9701-a7307777edd8",
   "metadata": {},
   "source": [
    "### Grid and cross-validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de3d30f8-2578-4b6a-b401-212d866c621d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "What's the average RMSE across the folds? (cross validation score)\n",
      "[1.2602868736515767]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A grid of parameter values (empty for the moment).\n",
    "params = ParamGridBuilder().build()\n",
    "\n",
    "# An object to evaluate model performance.\n",
    "evaluator_car = RegressionEvaluator(labelCol='consumption')\n",
    "\n",
    "# The cross-validation object.\n",
    "cv_car = CrossValidator(estimator=pipeline_car, estimatorParamMaps=params,\n",
    "                        evaluator=evaluator_car, numFolds=10, seed=SEED)\n",
    "\n",
    "# Apply cross-validation to the training data.\n",
    "cv_car = cv_car.fit(df_cars_train)\n",
    "\n",
    "# What's the average RMSE across the folds?\n",
    "print(f'''\n",
    "What's the average RMSE across the folds? (cross validation score)\n",
    "{cv_car.avgMetrics}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159d5b45-318a-4b8a-be77-ce679927368e",
   "metadata": {},
   "source": [
    "### Evaluationg on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32174159-ed44-4200-bc8a-3dea4f049afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RMSE: 1.2274630007114407\n",
      " MAE: 1.0808052770143333\n",
      "  R²: 0.6357997773880841\n",
      " MSE: 1.5066654181155341\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluating on the testing data\n",
    "predictions_cars = cv_car.transform(df_cars_test)\n",
    "\n",
    "eval_cars = RegressionEvaluator(labelCol=label_col)\n",
    "print(f'''\n",
    "RMSE: {eval_cars.evaluate(predictions_cars)}\n",
    " MAE: {eval_cars.evaluate(predictions_cars, {eval_cars.metricName: \"mae\"})}\n",
    "  R²: {eval_cars.evaluate(predictions_cars, {eval_cars.metricName: \"r2\"})}\n",
    " MSE: {eval_cars.evaluate(predictions_cars, {eval_cars.metricName: \"mse\"})}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f077c6-11da-4756-8ffc-2748ba360f98",
   "metadata": {},
   "source": [
    "## GridSearch\n",
    "\n",
    "### Manual process selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6da41695-2f80-4992-a1f4-3c6e7a757ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 75, Testing set: 17\n",
      "LinearRegression(fitIntercept=True) RMSE: 1.2274630007114407\n",
      "LinearRegression(fitIntercept=False) RMSE: 1.5719472988594767\n"
     ]
    }
   ],
   "source": [
    "# Loading the data\n",
    "df_cars = cars_data.select('*')\n",
    "\n",
    "# setting the label and features columns\n",
    "label_col = 'consumption'\n",
    "feature_cols = ['mass', 'cyl']\n",
    "\n",
    "# Setting the steps\n",
    "assemble_car = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "df_cars = assemble_car.transform(df_cars)\n",
    "\n",
    "# Evaluator \n",
    "evaluator_car = RegressionEvaluator(labelCol='consumption')\n",
    "\n",
    "# Split into train and test set.\n",
    "df_cars_train, df_cars_test = df_cars.randomSplit([0.8, 0.2], seed=SEED)\n",
    "print(f\"Training set: {df_cars_train.count()}, Testing set: {df_cars_test.count()}\")\n",
    "\n",
    "# Opcion fitIntercept=True\n",
    "regression_car = LinearRegression(labelCol=label_col, fitIntercept=True).fit(df_cars_train)\n",
    "predictions_cars = regression_car.transform(df_cars_test)\n",
    "print(f'''LinearRegression(fitIntercept=True) RMSE: {evaluator_car.evaluate(predictions_cars)}''')\n",
    "\n",
    "# Opcion fitIntercept=False\n",
    "regression_car = LinearRegression(labelCol=label_col, fitIntercept=False).fit(df_cars_train)\n",
    "predictions_cars = regression_car.transform(df_cars_test)\n",
    "print(f'''LinearRegression(fitIntercept=False) RMSE: {evaluator_car.evaluate(predictions_cars)}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85dbc62-bab4-4ace-8923-9cdc408705d9",
   "metadata": {},
   "source": [
    "### A simple Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27d0a815-b83b-45c4-99b1-833924f48e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 75, Testing set: 17\n",
      "Number of models to be tested:  2\n",
      "\n",
      "What's the cross-validated RMSE for each model? [1.2602868736515764, 1.415907832036895]\n",
      "\n",
      "Best Model: \n",
      "LinearRegressionModel: uid=LinearRegression_9315520dd3d6, numFeatures=2\n",
      "CV RMSE (best model): 1.2274630007114407\n",
      "Best parameter: \n",
      "fitIntercept: whether to fit an intercept term. (default: True, current: True)\n"
     ]
    }
   ],
   "source": [
    "# Loading the data\n",
    "df_cars = cars_data.select('*')\n",
    "\n",
    "# setting the label and features columns\n",
    "label_col = 'consumption'\n",
    "feature_cols = ['mass', 'cyl']\n",
    "\n",
    "# Setting the steps\n",
    "assemble_car = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "df_cars = assemble_car.transform(df_cars)\n",
    "\n",
    "# Split into train and test set.\n",
    "df_cars_train, df_cars_test = df_cars.randomSplit([0.8, 0.2], seed=SEED)\n",
    "print(f\"Training set: {df_cars_train.count()}, Testing set: {df_cars_test.count()}\")\n",
    "\n",
    "# Evaluator \n",
    "evaluator_car = RegressionEvaluator(labelCol='consumption')\n",
    "\n",
    "# Model\n",
    "regression_car = LinearRegression(labelCol=label_col)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Create a parameter grid builder\n",
    "params_car = ParamGridBuilder().addGrid(regression_car.fitIntercept, [True, False]).build()\n",
    "print('Number of models to be tested: ', len(params_car))\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Create a cross-validator and fit to the training data.\n",
    "cv_car = CrossValidator(estimator=regression_car, estimatorParamMaps=params_car,\n",
    "                    evaluator=evaluator_car)\n",
    "cv_car = cv_car.setNumFolds(10).setSeed(SEED).fit(df_cars_train)\n",
    "print(f'''\\nWhat's the cross-validated RMSE for each model? {cv_car.avgMetrics}''')\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Access the best model\n",
    "print(f'''\\nBest Model: \\n{cv_car.bestModel}''')\n",
    "\n",
    "# Or just use the cross-validator object.\n",
    "predictions_cars = cv_car.transform(df_cars_test)\n",
    "print(f'''CV RMSE (best model): {evaluator_car.evaluate(predictions_cars)}''')\n",
    "\n",
    "# Retrieve the best parameter.\n",
    "print(f'''Best parameter: \\n{cv_car.bestModel.explainParam(\"fitIntercept\")}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57a38a8-fad4-43ab-a38f-848362bad118",
   "metadata": {},
   "source": [
    "### A more complicated grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7fda7681-5aa5-4222-a187-315b5341677b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 75, Testing set: 17\n",
      "Number of models to be tested:  50\n"
     ]
    }
   ],
   "source": [
    "# Loading the data\n",
    "df_cars = cars_data.select('*')\n",
    "\n",
    "# setting the label and features columns\n",
    "label_col = 'consumption'\n",
    "feature_cols = ['mass', 'cyl']\n",
    "\n",
    "# Setting the steps\n",
    "assemble_car = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "df_cars = assemble_car.transform(df_cars)\n",
    "\n",
    "# Split into train and test set.\n",
    "df_cars_train, df_cars_test = df_cars.randomSplit([0.8, 0.2], seed=SEED)\n",
    "print(f\"Training set: {df_cars_train.count()}, Testing set: {df_cars_test.count()}\")\n",
    "\n",
    "# Evaluator \n",
    "evaluator_car = RegressionEvaluator(labelCol='consumption')\n",
    "\n",
    "# Model\n",
    "regression_car = LinearRegression(labelCol=label_col)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Create a parameter grid builder\n",
    "params_car = (ParamGridBuilder().addGrid(regression_car.fitIntercept, [True, False])\n",
    "                                .addGrid(regression_car.regParam, [0.001, 0.01, 0.1, 1, 10])\n",
    "                                .addGrid(regression_car.elasticNetParam, [0, 0.25, 0.5, 0.75, 1])\n",
    "                                .build())\n",
    "print('Number of models to be tested: ', len(params_car))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43011313-005f-46ed-92b8-acd2d63d6cf8",
   "metadata": {},
   "source": [
    "## Ensemble\n",
    "\n",
    "### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ad2de09-96be-497c-97cd-de82ec72b820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 75, Testing set: 17\n",
      "\n",
      "Trees in RandomForestClassifier: 5\n",
      "\n",
      "[DecisionTreeClassificationModel: uid=dtc_409dc29e8ae6, depth=5, numNodes=17, numClasses=2, numFeatures=6,\n",
      " DecisionTreeClassificationModel: uid=dtc_343b290d5ca9, depth=5, numNodes=17, numClasses=2, numFeatures=6,\n",
      " DecisionTreeClassificationModel: uid=dtc_2531f6c7fe2c, depth=5, numNodes=19, numClasses=2, numFeatures=6,\n",
      " DecisionTreeClassificationModel: uid=dtc_f6b53ed91162, depth=5, numNodes=19, numClasses=2, numFeatures=6,\n",
      " DecisionTreeClassificationModel: uid=dtc_99991c65db06, depth=4, numNodes=21, numClasses=2, numFeatures=6]\n",
      "\n",
      "Feature importance:\n",
      "       Feature  Importance\n",
      "4          rpm    0.301180\n",
      "3       length    0.264662\n",
      "1         size    0.152525\n",
      "5  consumption    0.143760\n",
      "2         mass    0.129507\n",
      "0          cyl    0.008366\n",
      "\n",
      "\n",
      "Reviewing predictions (5 first records):\n",
      "+-----+----------------------------------------+----------+\n",
      "|label|probability                             |prediction|\n",
      "+-----+----------------------------------------+----------+\n",
      "|1.0  |[0.41052631578947374,0.5894736842105263]|1.0       |\n",
      "|0.0  |[1.0,0.0]                               |0.0       |\n",
      "|0.0  |[1.0,0.0]                               |0.0       |\n",
      "|0.0  |[1.0,0.0]                               |0.0       |\n",
      "|0.0  |[0.8,0.2]                               |0.0       |\n",
      "+-----+----------------------------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Accuracy : 0.8235294117647058\n",
      "\n",
      "Confussion Matrix:\n",
      "                     label  prediction  count\n",
      "True negative (TN)     0.0         0.0      8\n",
      "False negative (FN)    1.0         0.0      1\n",
      "False positive (FP)    0.0         1.0      2\n",
      "True positive (TP)     1.0         1.0      6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading the data\n",
    "df_cars = cars_data.select('*')\n",
    "df_cars = StringIndexer(inputCol=\"origin\", outputCol=\"label\").fit(df_cars).transform(df_cars)\n",
    "\n",
    "# setting the label and features columns\n",
    "feature_cols = ['cyl', 'size', 'mass', 'length', 'rpm', 'consumption']\n",
    "\n",
    "# Setting the steps\n",
    "assemble_car = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "df_cars = assemble_car.transform(df_cars)\n",
    "\n",
    "# Split into train and test set.\n",
    "df_cars_train, df_cars_test = df_cars.randomSplit([0.8, 0.2], seed=SEED)\n",
    "print(f\"Training set: {df_cars_train.count()}, Testing set: {df_cars_test.count()}\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Create a forest of trees\n",
    "forest_car = RandomForestClassifier(numTrees=5, seed=SEED).fit(df_cars_train)\n",
    "\n",
    "# Seeing the trees\n",
    "print(f'''\n",
    "Trees in RandomForestClassifier: {len(forest_car.trees)}\n",
    "''')\n",
    "pprint(forest_car.trees)\n",
    "\n",
    "# Feature importances\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Importance': DenseVector(forest_car.featureImportances)\n",
    "})\n",
    "print(f'''\n",
    "Feature importance:\n",
    "{feature_importance.sort_values('Importance', ascending=False)}\n",
    "''')\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Consensus predictions\n",
    "predictions_car = forest_car.transform(df_cars_test)\n",
    "print('\\nReviewing predictions (5 first records):')\n",
    "predictions_car.select('label', 'probability', 'prediction').show(5, truncate=False)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Evaluation\n",
    "cm_car = predictions_car.groupBy(\"label\", \"prediction\").count().toPandas().sort_values([\"prediction\", \"label\"])\n",
    "cm_car.index = ['True negative (TN)', 'False negative (FN)', 'False positive (FP)', 'True positive (TP)']\n",
    "TN, FN, FP, TP = cm_car['count'].to_list()\n",
    "accuracy_forest_car = (TN + TP) / (TN + TP + FN + FP)\n",
    "print(f'''\n",
    "Accuracy : {accuracy_forest_car}\n",
    "\n",
    "Confussion Matrix:\n",
    "{cm_car}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bf01d3-7ffc-4809-b49e-fad7696d1019",
   "metadata": {},
   "source": [
    "### Gradient-Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c282d549-accc-45b7-8dc2-41003839df12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 75, Testing set: 17\n",
      "\n",
      "Trees in Gradient-Boosted Tree: 10\n",
      "\n",
      "[DecisionTreeRegressionModel: uid=dtr_380c018ebed1, depth=5, numNodes=21, numFeatures=6,\n",
      " DecisionTreeRegressionModel: uid=dtr_79814dddc780, depth=5, numNodes=33, numFeatures=6,\n",
      " DecisionTreeRegressionModel: uid=dtr_8230e0062dcb, depth=5, numNodes=37, numFeatures=6,\n",
      " DecisionTreeRegressionModel: uid=dtr_a65a69a496c8, depth=5, numNodes=37, numFeatures=6,\n",
      " DecisionTreeRegressionModel: uid=dtr_eb902d1b3291, depth=5, numNodes=41, numFeatures=6,\n",
      " DecisionTreeRegressionModel: uid=dtr_651efa8de78f, depth=5, numNodes=35, numFeatures=6,\n",
      " DecisionTreeRegressionModel: uid=dtr_dd23212651b6, depth=5, numNodes=35, numFeatures=6,\n",
      " DecisionTreeRegressionModel: uid=dtr_169b50283d65, depth=5, numNodes=29, numFeatures=6,\n",
      " DecisionTreeRegressionModel: uid=dtr_c093dbb701a2, depth=5, numNodes=29, numFeatures=6,\n",
      " DecisionTreeRegressionModel: uid=dtr_2ba15baf8525, depth=5, numNodes=35, numFeatures=6]\n",
      "\n",
      "Feature importance:\n",
      "       Feature  Importance\n",
      "4          rpm    0.285825\n",
      "3       length    0.239765\n",
      "2         mass    0.230588\n",
      "5  consumption    0.127953\n",
      "1         size    0.115848\n",
      "0          cyl    0.000022\n",
      "\n",
      "\n",
      "Accuracy : 0.8235294117647058\n",
      "\n",
      "Confussion Matrix:\n",
      "                     label  prediction  count\n",
      "True negative (TN)     0.0         0.0      8\n",
      "False negative (FN)    1.0         0.0      1\n",
      "False positive (FP)    0.0         1.0      2\n",
      "True positive (TP)     1.0         1.0      6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading the data\n",
    "df_cars = cars_data.select('*')\n",
    "df_cars = StringIndexer(inputCol=\"origin\", outputCol=\"label\").fit(df_cars).transform(df_cars)\n",
    "\n",
    "# setting the label and features columns\n",
    "feature_cols = ['cyl', 'size', 'mass', 'length', 'rpm', 'consumption']\n",
    "\n",
    "# Setting the steps\n",
    "assemble_car = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "df_cars = assemble_car.transform(df_cars)\n",
    "\n",
    "# Split into train and test set.\n",
    "df_cars_train, df_cars_test = df_cars.randomSplit([0.8, 0.2], seed=SEED)\n",
    "print(f\"Training set: {df_cars_train.count()}, Testing set: {df_cars_test.count()}\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Create a Gradient-Boosted Tree classifier\n",
    "gbt_car = GBTClassifier(maxIter=10, seed=SEED).fit(df_cars_train)\n",
    "\n",
    "# Seeing the trees\n",
    "print(f'''\n",
    "Trees in Gradient-Boosted Tree: {len(gbt_car.trees)}\n",
    "''')\n",
    "pprint(gbt_car.trees)\n",
    "\n",
    "# Feature importances\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Importance': DenseVector(gbt_car.featureImportances)\n",
    "})\n",
    "print(f'''\n",
    "Feature importance:\n",
    "{feature_importance.sort_values('Importance', ascending=False)}\n",
    "''')\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Predictions\n",
    "predictions_car = gbt_car.transform(df_cars_test)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Evaluation\n",
    "cm_car = predictions_car.groupBy(\"label\", \"prediction\").count().toPandas().sort_values([\"prediction\", \"label\"])\n",
    "cm_car.index = ['True negative (TN)', 'False negative (FN)', 'False positive (FP)', 'True positive (TP)']\n",
    "TN, FN, FP, TP = cm_car['count'].to_list()\n",
    "accuracy_gbt_car = (TN + TP) / (TN + TP + FN + FP)\n",
    "print(f'''\n",
    "Accuracy : {accuracy_gbt_car}\n",
    "\n",
    "Confussion Matrix:\n",
    "{cm_car}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef06020-943d-4410-90f2-b55778e40df2",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8f9a5a1-4039-4fa4-a6d8-d99e9c2c4584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 75, Testing set: 17\n",
      "\n",
      "Feature importance:\n",
      "       Feature  Importance\n",
      "4          rpm    0.470220\n",
      "3       length    0.233508\n",
      "2         mass    0.109938\n",
      "5  consumption    0.105161\n",
      "1         size    0.081173\n",
      "0          cyl    0.000000\n",
      "\n",
      "\n",
      "Accuracy : 0.8235294117647058\n",
      "\n",
      "Confussion Matrix:\n",
      "                     label  prediction  count\n",
      "True negative (TN)     0.0         0.0      9\n",
      "False negative (FN)    1.0         0.0      2\n",
      "False positive (FP)    0.0         1.0      1\n",
      "True positive (TP)     1.0         1.0      5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading the data\n",
    "df_cars = cars_data.select('*')\n",
    "df_cars = StringIndexer(inputCol=\"origin\", outputCol=\"label\").fit(df_cars).transform(df_cars)\n",
    "\n",
    "# setting the label and features columns\n",
    "feature_cols = ['cyl', 'size', 'mass', 'length', 'rpm', 'consumption']\n",
    "\n",
    "# Setting the steps\n",
    "assemble_car = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "df_cars = assemble_car.transform(df_cars)\n",
    "\n",
    "# Split into train and test set.\n",
    "df_cars_train, df_cars_test = df_cars.randomSplit([0.8, 0.2], seed=SEED)\n",
    "print(f\"Training set: {df_cars_train.count()}, Testing set: {df_cars_test.count()}\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Create a Decision Tree model\n",
    "tree_car = DecisionTreeClassifier(seed=SEED).fit(df_cars_train)\n",
    "\n",
    "# Feature importances\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Importance': DenseVector(tree_car.featureImportances)\n",
    "})\n",
    "print(f'''\n",
    "Feature importance:\n",
    "{feature_importance.sort_values('Importance', ascending=False)}\n",
    "''')\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Predictions\n",
    "predictions_car = tree_car.transform(df_cars_test)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Evaluation\n",
    "cm_car = predictions_car.groupBy(\"label\", \"prediction\").count().toPandas().sort_values([\"prediction\", \"label\"])\n",
    "cm_car.index = ['True negative (TN)', 'False negative (FN)', 'False positive (FP)', 'True positive (TP)']\n",
    "TN, FN, FP, TP = cm_car['count'].to_list()\n",
    "accuracy_tree_car = (TN + TP) / (TN + TP + FN + FP)\n",
    "print(f'''\n",
    "Accuracy : {accuracy_tree_car}\n",
    "\n",
    "Confussion Matrix:\n",
    "{cm_car}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cad1483-a0a4-49eb-a0d5-3ed5c932cbe1",
   "metadata": {},
   "source": [
    "### Comparing trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df6aa0da-2265-4342-b2b8-7e8df7834b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RandomForestClassifier: 0.8235294117647058\n",
      "  GradientBoostedTrees: 0.8235294117647058\n",
      "          DecisionTree: 0.8235294117647058\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'''\n",
    "RandomForestClassifier: {accuracy_forest_car}\n",
    "  GradientBoostedTrees: {accuracy_gbt_car}\n",
    "          DecisionTree: {accuracy_tree_car}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d27da0-4834-4f3e-b289-f65c8ab42fdd",
   "metadata": {},
   "source": [
    "# Fligths Dataset\n",
    "\n",
    "## Pipeline\n",
    "\n",
    "### Ex. 1 - Flight duration model: Pipeline stages\n",
    "\n",
    "You're going to create the stages for the flights duration model pipeline. You will use these in the next exercise to build a pipeline and to create a regression model.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Create an indexer to convert the `org` column into an indexed column called `org_idx`.\n",
    "2. Create a one-hot encoder to convert the `org_idx` and `dow` columns into vec variable columns called `org_vec` and `dow_vec`.\n",
    "3. Create an assembler which will combine the `km` column with the two vec variable columns. The output column should be called `features`.\n",
    "4. Create a linear regression object to predict flight `duration`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "67c2d081-0760-4ceb-a9f2-1afca8e91b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 37601, Testing set: 9421\n"
     ]
    }
   ],
   "source": [
    "# Loading the data\n",
    "df_flights = flights_data.select('*')\n",
    "\n",
    "# setting the label and features columns\n",
    "label_col = 'duration'\n",
    "feature_cols = ['km', 'org_vec', 'dow_vec']\n",
    "\n",
    "# Setting the steps\n",
    "indexer_flight = StringIndexer(inputCol='org', outputCol='org_idx')\n",
    "onehot_flight = OneHotEncoder(inputCols=['org_idx', 'dow'], outputCols=['org_vec', 'dow_vec'])\n",
    "assemble_flight = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "regression_flight = LinearRegression(labelCol=label_col)\n",
    "\n",
    "# Split into train and test set.\n",
    "df_flights_train, df_flights_test = df_flights.randomSplit([0.8, 0.2], seed=SEED)\n",
    "print(f\"Training set: {df_flights_train.count()}, Testing set: {df_flights_test.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b0e7b6-61db-490a-89a7-8d5b286d31d1",
   "metadata": {},
   "source": [
    "### Ex. 2 - Flight duration model: Pipeline model\n",
    "\n",
    "You're now ready to put those stages together in a pipeline.\n",
    "\n",
    "You'll construct the pipeline and then train the pipeline on the training data. This will apply each of the individual stages in the pipeline to the training data in turn. None of the stages will be exposed to the testing data at all: there will be no leakage!\n",
    "\n",
    "Once the entire pipeline has been trained it will then be used to make predictions on the testing data.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Import the class for creating a pipeline.\n",
    "2. Create a pipeline object and specify the indexer, onehot, assembler and regression stages, in this order.\n",
    "3. Train the pipeline on the training data.\n",
    "4. Make predictions on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f9d6fa9-bf35-4cf1-88a0-13743eb6e5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RMSE: 11.018250570030013\n",
      " MAE: 8.531036191848083\n",
      "  R²: 0.9841835312882599\n",
      " MSE: 121.40184562396668\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols_to_predict = ['org_idx', 'org_vec', 'dow_vec', 'features', 'prediction']\n",
    "df_flights_train = df_flights_train.drop(*cols_to_predict)\n",
    "df_flights_test = df_flights_test.drop(*cols_to_predict)\n",
    "label_col = 'duration'\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline_flight = Pipeline(stages=[indexer_flight, onehot_flight, assemble_flight, regression_flight])\n",
    "\n",
    "# Fitting the model on the trainning data\n",
    "pipeline_flight = pipeline_flight.fit(df_flights_train)\n",
    "df_flights_train = pipeline_flight.transform(df_flights_train)\n",
    "\n",
    "# Evaluating on the testing data\n",
    "predictions_flights = pipeline_flight.transform(df_flights_test)\n",
    "\n",
    "eval_flights = RegressionEvaluator(labelCol=label_col)\n",
    "print(f'''\n",
    "RMSE: {eval_flights.evaluate(predictions_flights)}\n",
    " MAE: {eval_flights.evaluate(predictions_flights, {eval_flights.metricName: \"mae\"})}\n",
    "  R²: {eval_flights.evaluate(predictions_flights, {eval_flights.metricName: \"r2\"})}\n",
    " MSE: {eval_flights.evaluate(predictions_flights, {eval_flights.metricName: \"mse\"})}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb823378-8aa8-4596-8a74-3300d1cc5e93",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n",
    "### Ex. 4 - Cross validating simple flight duration model\n",
    "\n",
    "You've already built a few models for predicting flight duration and evaluated them with a simple train/test split. However, cross-validation provides a much better way to evaluate model performance.\n",
    "\n",
    "In this exercise you're going to train a simple model for flight duration using cross-validation. Travel time is usually strongly correlated with distance, so using the km column alone should give a decent model.\n",
    "\n",
    "**Instructions:**\n",
    "1. Create an empty parameter grid.\n",
    "2. Create objects for building and evaluating a linear regression model. The model should predict the \"duration\" field.\n",
    "3. Create a cross-validator object. Provide values for the estimator, estimatorParamMaps and evaluator arguments. Choose 5-fold cross validation.\n",
    "4. Train and test the model across multiple folds of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0ceb7196-8b6f-4b65-a37d-d8630a247683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "What's the average RMSE across the folds? (cross validation score)\n",
      "[11.043356149070807]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols_to_predict = ['org_idx', 'org_vec', 'dow_vec', 'features', 'prediction']\n",
    "df_flights_train = df_flights_train.drop(*cols_to_predict)\n",
    "df_flights_test = df_flights_test.drop(*cols_to_predict)\n",
    "label_col = 'duration'\n",
    "\n",
    "# Define the estimator\n",
    "pipeline_flight = Pipeline(stages=[indexer_flight, onehot_flight, assemble_flight, regression_flight])\n",
    "\n",
    "# A grid of parameter values (empty for the moment).\n",
    "params = ParamGridBuilder().build()\n",
    "\n",
    "# An object to evaluate model performance.\n",
    "evaluator_flight = RegressionEvaluator(labelCol='duration')\n",
    "\n",
    "# The cross-validation object.\n",
    "cv_flight = CrossValidator(estimator=pipeline_flight, estimatorParamMaps=params,\n",
    "                           evaluator=evaluator_flight, numFolds=10, seed=SEED)\n",
    "\n",
    "# Apply cross-validation to the training data.\n",
    "cv_flight = cv_flight.fit(df_flights_train)\n",
    "\n",
    "# What's the average RMSE across the folds?\n",
    "print(f'''\n",
    "What's the average RMSE across the folds? (cross validation score)\n",
    "{cv_flight.avgMetrics}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250b2310-7760-4593-8532-b1f135907c78",
   "metadata": {},
   "source": [
    "### Ex. 5 - Cross validating flight duration model pipeline\n",
    "\n",
    "The cross-validated model that you just built was simple, using `km` alone to predict duration.\n",
    "\n",
    "Another important predictor of flight duration is the origin airport. Flights generally take longer to get into the air from busy airports. Let's see if adding this predictor improves the model!\n",
    "\n",
    "In this exercise you'll add the `org` field to the model. However, since `org` is categorical, there's more work to be done before it can be included: it must first be transformed to an index and then one-hot encoded before being assembled with `km` and used to build the regression model. We'll wrap these operations up in a pipeline.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Create a string indexer. Specify the input and output fields as org and org_idx.\n",
    "2. Create a one-hot encoder. Name the output field org_dummy.\n",
    "3. Assemble the km and org_dummy fields into a single field called features.\n",
    "4. Create a pipeline using the following operations: string indexer, one-hot encoder, assembler and linear regression. Use this to create a cross-validator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d9888d30-da06-40d4-a100-d839c95b419e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 37601, Testing set: 9421\n",
      "\n",
      "What's the average RMSE across the folds? (cross validation score)\n",
      "[11.043356149070807]\n",
      "\n",
      "\n",
      "RMSE: 11.018250570030013\n",
      " MAE: 8.531036191848083\n",
      "  R²: 0.9841835312882599\n",
      " MSE: 121.40184562396668\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading the data\n",
    "df_flights = flights_data.select('*')\n",
    "\n",
    "# setting the label and features columns\n",
    "label_col = 'duration'\n",
    "feature_cols = ['km', 'org_vec']\n",
    "\n",
    "# Setting the steps\n",
    "indexer_flight = StringIndexer(inputCol='org', outputCol='org_idx')\n",
    "onehot_flight = OneHotEncoder(inputCols=[indexer_flight.getOutputCol()], outputCols=['org_vec'])\n",
    "assemble_flight = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "regression_flight = LinearRegression(labelCol=label_col)\n",
    "\n",
    "# Split into train and test set.\n",
    "df_flights_train, df_flights_test = df_flights.randomSplit([0.8, 0.2], seed=SEED)\n",
    "print(f\"Training set: {df_flights_train.count()}, Testing set: {df_flights_test.count()}\")\n",
    "\n",
    "# A grid of parameter values (empty for the moment).\n",
    "params = ParamGridBuilder().build()\n",
    "\n",
    "# An object to evaluate model performance.\n",
    "evaluator_flight = RegressionEvaluator(labelCol=label_col)\n",
    "\n",
    "# The cross-validation object.\n",
    "cv_flight = CrossValidator(estimator=pipeline_flight, estimatorParamMaps=params,\n",
    "                           evaluator=evaluator_flight, numFolds=10, seed=SEED)\n",
    "\n",
    "# Apply cross-validation to the training data.\n",
    "cv_flight = cv_flight.fit(df_flights_train)\n",
    "\n",
    "# What's the average RMSE across the folds?\n",
    "print(f'''\n",
    "What's the average RMSE across the folds? (cross validation score)\n",
    "{cv_flight.avgMetrics}\n",
    "''')\n",
    "\n",
    "# Evaluating on the testing data\n",
    "predictions_flights = cv_flight.transform(df_flights_test)\n",
    "\n",
    "print(f'''\n",
    "RMSE: {evaluator_flight.evaluate(predictions_flights)}\n",
    " MAE: {evaluator_flight.evaluate(predictions_flights, {evaluator_flight.metricName: \"mae\"})}\n",
    "  R²: {evaluator_flight.evaluate(predictions_flights, {evaluator_flight.metricName: \"r2\"})}\n",
    " MSE: {evaluator_flight.evaluate(predictions_flights, {evaluator_flight.metricName: \"mse\"})}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658cd977-025c-47a4-ae2f-44525086ca44",
   "metadata": {},
   "source": [
    "## GridSearch\n",
    "\n",
    "### Ex. 6 - Optimizing flights linear regression\n",
    "\n",
    "Up until now you've been using the default hyper-parameters when building your models. In this exercise you'll use cross validation to choose an optimal (or close to optimal) set of model hyper-parameters.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Create a parameter grid builder.\n",
    "2. Add grids for with regression.regParam (values `0.01`, `0.1`, `1.0`, and `10.0`) and regression.`elasticNetParam` (values `0.0`, `0.5`, and `1.0`).\n",
    "3. Build the grid.\n",
    "4. Create a cross validator, specifying five folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d1ec32f-d485-48a9-b703-64299f7c4048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 37601, Testing set: 9421\n",
      "Number of models to be tested:  12\n"
     ]
    }
   ],
   "source": [
    "# Loading the data\n",
    "df_flights = flights_data.select('*')\n",
    "\n",
    "# setting the label and features columns\n",
    "label_col = 'duration'\n",
    "feature_cols = ['km', 'org_vec']\n",
    "\n",
    "# Setting the steps\n",
    "indexer_flight = StringIndexer(inputCol='org', outputCol='org_idx')\n",
    "onehot_flight = OneHotEncoder(inputCols=[indexer_flight.getOutputCol()], outputCols=['org_vec'])\n",
    "assemble_flight = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "regression_flight = LinearRegression(labelCol=label_col)\n",
    "\n",
    "# Define the estimator\n",
    "pipeline_flight = Pipeline(stages=[indexer_flight, onehot_flight, assemble_flight, regression_flight])\n",
    "\n",
    "# Split into train and test set.\n",
    "df_flights_train, df_flights_test = df_flights.randomSplit([0.8, 0.2], seed=SEED)\n",
    "print(f\"Training set: {df_flights_train.count()}, Testing set: {df_flights_test.count()}\")\n",
    "\n",
    "# A grid of parameter values (empty for the moment).\n",
    "params = (ParamGridBuilder().addGrid(regression_flight.regParam, [0.01, 0.1, 1.0, 10.0])\n",
    "                            .addGrid(regression_flight.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "                            .build())\n",
    "print('Number of models to be tested: ', len(params))\n",
    "\n",
    "# An object to evaluate model performance.\n",
    "evaluator_flight = RegressionEvaluator(labelCol=label_col)\n",
    "\n",
    "# The cross-validation object.\n",
    "cv_flight = CrossValidator(estimator=pipeline_flight, estimatorParamMaps=params,\n",
    "                           evaluator=evaluator_flight, numFolds=10, seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc283a4-d75c-4d18-93bd-2eaa4bd1f463",
   "metadata": {},
   "source": [
    "### Ex. 7 - Dissecting the best flight duration model\n",
    "\n",
    "You just set up a `CrossValidator` to find good parameters for the linear regression model predicting flight `duration`.\n",
    "\n",
    "The model pipeline has multiple stages (objects of type `StringIndexer`, `OneHotEncoder`, `VectorAssembler` and `LinearRegression`), which operate in sequence. The stages are available as the stages attribute on the pipeline object. They are represented by a list and the stages are executed in the sequence in which they appear in the list.\n",
    "\n",
    "Now you're going to take a closer look at the pipeline, split out the stages and use it to make predictions on the testing data.\n",
    "\n",
    "The following objects have already been created:\n",
    "- `cv_flight` — a trained CrossValidatorModel object and\n",
    "- `evaluator_flight` — a RegressionEvaluator object.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Retrieve the best model.\n",
    "2. Look at the stages in the best model.\n",
    "3. Isolate the linear regression stage and extract its parameters.\n",
    "4. Use the best model to generate predictions on the testing data and calculate the RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f4ef50d5-06b8-47f1-9cdd-32c8a9fdc91e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's the cross-validated RMSE for each model? [11.043796482476411, 11.044215367880962, 11.044972888958773, 11.04569240725167, 11.07688496687515, 11.151202862483116, 11.168330585629565, 11.515881741405666, 11.693665918948176, 14.536143664571739, 17.014857807938384, 19.16091783706862]\n",
      "Best Model: \n",
      "PipelineModel_515019dbe135\n",
      "\n",
      "LinearRegression Parameters (best model): \n",
      "best_model.stages[3].extractParamMap()\n",
      "\n",
      "RMSE = 11.017107718193145\n"
     ]
    }
   ],
   "source": [
    "# Apply cross-validation to the training data.\n",
    "cv_flight = cv_flight.fit(df_flights_train)\n",
    "print(f\"What's the cross-validated RMSE for each model? {cv_flight.avgMetrics}\")\n",
    "\n",
    "# Access the best model\n",
    "best_model = cv_flight.bestModel\n",
    "print(f'Best Model: \\n{best_model}')\n",
    "\n",
    "# Get the parameters for the LinearRegression object in the best model\n",
    "print(f'''\n",
    "LinearRegression Parameters (best model): \n",
    "best_model.stages[3].extractParamMap()\n",
    "''')\n",
    "\n",
    "# Generate predictions on testing data using the best model then calculate RMSE\n",
    "predictions_flights = best_model.transform(df_flights_test)\n",
    "print(\"RMSE =\", evaluator_flight.evaluate(predictions_flights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94593b6d-87ee-4cc3-af9a-2b22a57c718f",
   "metadata": {},
   "source": [
    "## Ensemble\n",
    "\n",
    "### Ex. 10 - Delayed flights with Gradient-Boosted Trees\n",
    "\n",
    "You've previously built a classifier for flights likely to be delayed using a Decision Tree. In this exercise you'll compare a Decision Tree model to a Gradient-Boosted Trees model.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Import the classes required to create Decision Tree and Gradient-Boosted Tree classifiers.\n",
    "2. Create Decision Tree and Gradient-Boosted Tree classifiers. Train on the training data.\n",
    "3. Create an evaluator and calculate AUC on testing data for both classifiers. Which model performs better?\n",
    "4. For the Gradient-Boosted Tree classifier print the number of trees and the relative importance of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "36bbad61-e25c-4b86-a59a-fd4a889fbf4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 37601, Testing set: 9421\n",
      "\n",
      "Feature importance:\n",
      "    Feature  Importance GBT  Importance Tree\n",
      "0       mon        0.306272         0.365375\n",
      "1    depart        0.326609         0.424626\n",
      "2  duration        0.367119         0.209999\n",
      "\n",
      "\n",
      "Accuracy GBT: 0.6764592767631815\n",
      "Accuracy Tree: 0.6177641355453237\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading the data\n",
    "df_flights = flights_data.select('*')\n",
    "df_flights = df_flights.withColumn('label', (df_flights['delay']>=15).cast('integer'))\n",
    "\n",
    "# setting the label and features columns\n",
    "feature_cols = ['mon', 'depart', 'duration']\n",
    "\n",
    "# Setting the steps\n",
    "assemble_flight = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "df_flights = assemble_flight.transform(df_flights)\n",
    "\n",
    "# Split into train and test set.\n",
    "df_flights_train, df_flights_test = df_flights.randomSplit([0.8, 0.2], seed=SEED)\n",
    "print(f\"Training set: {df_flights_train.count()}, Testing set: {df_flights_test.count()}\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Create the classifier model\n",
    "gbt_flight = GBTClassifier(seed=SEED).fit(df_flights_train)\n",
    "tree_flight = DecisionTreeClassifier(seed=SEED).fit(df_flights_train)\n",
    "\n",
    "# Feature importances\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Importance GBT': DenseVector(gbt_flight.featureImportances),\n",
    "    'Importance Tree': DenseVector(tree_flight.featureImportances)\n",
    "})\n",
    "print(f'''\n",
    "Feature importance:\n",
    "{feature_importance}\n",
    "''')\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Predictions\n",
    "predictions_flight_gbt = gbt_flight.transform(df_flights_test)\n",
    "predictions_flight_tree = tree_flight.transform(df_flights_test)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Evaluation\n",
    "evaluator_flight = BinaryClassificationEvaluator()\n",
    "print(f'''\n",
    "Accuracy GBT: {evaluator_flight.evaluate(gbt_flight.transform(df_flights_test))}\n",
    "Accuracy Tree: {evaluator_flight.evaluate(tree_flight.transform(df_flights_test))}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4eac7bd-da1e-4d6a-b4f0-391a30601c43",
   "metadata": {},
   "source": [
    "### Ex. 11 - Delayed flights with a Random Forest\n",
    "\n",
    "In this exercise you'll bring together cross validation and ensemble methods. You'll be training a Random Forest classifier to predict delayed flights, using cross validation to choose the best values for model parameters.\n",
    "\n",
    "You'll find good values for the following parameters:\n",
    "- `featureSubsetStrategy` — the number of features to consider for splitting at each node and\n",
    "- `maxDepth` — the maximum number of splits along any branch.\n",
    "\n",
    "Unfortunately building this model takes too long, so we won't be running the `.fit()` method on the pipeline.\n",
    "\n",
    "The `RandomForestClassifier` class has already been imported into the session.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Create a random forest classifier object.\n",
    "2. Create a parameter grid builder object. Add grid points for the featureSubsetStrategy and maxDepth parameters.\n",
    "3. Create binary classification evaluator.\n",
    "4. Create a cross-validator object, specifying the estimator, parameter grid and evaluator. Choose 5-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c8fa30c8-dd23-49d5-ba1c-c8b57edbdd80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models in GridSearch:  12\n"
     ]
    }
   ],
   "source": [
    "# Create a random forest classifier\n",
    "forest_flight = RandomForestClassifier()\n",
    "\n",
    "# Create a parameter grid\n",
    "params = (ParamGridBuilder().addGrid(forest_flight.featureSubsetStrategy, ['all', 'onethird', 'sqrt', 'log2'])\n",
    "                            .addGrid(forest_flight.maxDepth, [2, 5, 10])\n",
    "                            .build())\n",
    "print('Number of models in GridSearch: ', len(params))\n",
    "\n",
    "# Create a binary classification evaluator\n",
    "evaluator_flight = BinaryClassificationEvaluator()\n",
    "\n",
    "# Create a cross-validator\n",
    "cv_flight = CrossValidator(estimator=forest_flight, \n",
    "                           estimatorParamMaps=params, \n",
    "                           evaluator=evaluator_flight, \n",
    "                           numFolds=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e38c83-3668-46ab-bd54-12add01163ae",
   "metadata": {},
   "source": [
    "### Ex. 12 - Evaluating Random Forest\n",
    "\n",
    "In this final exercise you'll be evaluating the results of cross-validation on a Random Forest model.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Print a list of average AUC metrics across all models in the parameter grid.\n",
    "2. Display the average AUC for the best model. This will be the largest AUC in the list.\n",
    "3. Print an explanation of the maxDepth and featureSubsetStrategy parameters for the best model.\n",
    "4. Display the AUC for the best model predictions on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e43e0207-2fb8-4ac6-967e-4c118fe1e040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 37601, Testing set: 9421\n",
      "\n",
      "Average AUC for each parameter combination in grid: \n",
      "[0.6190417566601683, 0.6608697791808258, 0.6698831701918285, 0.646538142319198, 0.6636898339808173, 0.6736555609859828, 0.6420409970736392, 0.6644620268396466, 0.6709851288402727, 0.6420409970736392, 0.6644620268396466, 0.6709851288402727]\n",
      "\n",
      "Average AUC for the best model: 0.6736555609859828\n",
      "\n",
      "What's the optimal parameter value for maxDepth?\n",
      "maxDepth: Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30]. (default: 5, current: 10)\n",
      "\n",
      "What's the optimal parameter value for featureSubsetStrategy?\n",
      "featureSubsetStrategy: The number of features to consider for splits at each tree node. Supported options: 'auto' (choose automatically for task: If numTrees == 1, set to 'all'. If numTrees > 1 (forest), set to 'sqrt' for classification and to 'onethird' for regression), 'all' (use all features), 'onethird' (use 1/3 of the features), 'sqrt' (use sqrt(number of features)), 'log2' (use log2(number of features)), 'n' (when n is in the range (0, 1.0], use n * number of features. When n is in the range (1, number of features), use n features). default = 'auto' (default: auto, current: onethird)\n",
      "\n",
      "AUC for best model on testing data:\n",
      "0.6758223911260522\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading the data\n",
    "df_flights = flights_data.select('*')\n",
    "df_flights = df_flights.withColumn('label', (df_flights['delay']>=15).cast('integer'))\n",
    "\n",
    "# setting the label and features columns\n",
    "feature_cols = ['mon', 'depart', 'duration']\n",
    "\n",
    "# Setting the steps\n",
    "assemble_flight = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "df_flights = assemble_flight.transform(df_flights)\n",
    "\n",
    "# Split into train and test set.\n",
    "df_flights_train, df_flights_test = df_flights.randomSplit([0.8, 0.2], seed=SEED)\n",
    "print(f\"Training set: {df_flights_train.count()}, Testing set: {df_flights_test.count()}\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Fitting the model\n",
    "cv_flight = cv_flight.fit(df_flights_train)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Inspecting results\n",
    "print(f'''\n",
    "Average AUC for each parameter combination in grid: \n",
    "{cv_flight.avgMetrics}\n",
    "\n",
    "Average AUC for the best model: {max(cv_flight.avgMetrics)}\n",
    "\n",
    "What's the optimal parameter value for maxDepth?\n",
    "{cv_flight.bestModel.explainParam('maxDepth')}\n",
    "\n",
    "What's the optimal parameter value for featureSubsetStrategy?\n",
    "{cv_flight.bestModel.explainParam('featureSubsetStrategy')}\n",
    "\n",
    "AUC for best model on testing data:\n",
    "{evaluator_flight.evaluate(cv_flight.transform(df_flights_test))}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54b969a-6ffd-4370-a719-1b1ad09e7b67",
   "metadata": {},
   "source": [
    "# SMS Datasets\n",
    "\n",
    "## Ex. 3 - SMS spam pipeline\n",
    "\n",
    "You haven't looked at the SMS data for quite a while. Last time we did the following:\n",
    "- split the text into tokens\n",
    "- removed stop words\n",
    "- applied the hashing trick\n",
    "- converted the data from counts to IDF and\n",
    "- trained a logistic regression model.\n",
    "Each of these steps was done independently. This seems like a great application for a pipeline!\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Create an object for splitting text into tokens.\n",
    "2. Create an object to remove stop words. Rather than explicitly giving the input column name, use the `getOutputCol()` method on the previous object.\n",
    "3. Create objects for applying the hashing trick and transforming the data into a TF-IDF. Use the `getOutputCol()` method again.\n",
    "4. Create a pipeline which wraps all of the above steps as well as an object to create a Logistic Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9181ab09-38dd-4170-ae5b-74a7f22434d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 4503, Testing set: 1071\n"
     ]
    }
   ],
   "source": [
    "# Loading the data\n",
    "df_sms = sms_data.select('*')\n",
    "\n",
    "# setting the label and features columns\n",
    "label_col = 'label'\n",
    "feature_cols = [\"hash\"]\n",
    "\n",
    "# Setting the steps\n",
    "tokenizer_sms = Tokenizer(inputCol='text', outputCol='words') # Break text into tokens at non-word characters\n",
    "remover_sms = StopWordsRemover(inputCol=tokenizer_sms.getOutputCol(), outputCol='terms') # Remove stop words\n",
    "hasher_sms = HashingTF(inputCol=remover_sms.getOutputCol(), outputCol=\"hash\") # Hashing trick, transform to TF-IDF\n",
    "idf_sms = IDF(inputCol=hasher_sms.getOutputCol(), outputCol=\"features\")\n",
    "logistic_sms = LogisticRegression() # Create a logistic regression object\n",
    "\n",
    "# Split into train and test set.\n",
    "df_sms_train, df_sms_test = df_sms.randomSplit([0.8, 0.2], seed=SEED)\n",
    "print(f\"Training set: {df_sms_train.count()}, Testing set: {df_sms_test.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3dded74c-c6b1-4367-88f8-f832c8a24470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy : 0.976657329598506\n",
      "Precision: 0.9534883720930233\n",
      "Recall   : 0.8661971830985915\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>prediction</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True negative (TN)</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False negative (FN)</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False positive (FP)</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True positive (TP)</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     label  prediction  count\n",
       "True negative (TN)       0         0.0    923\n",
       "False negative (FN)      1         0.0     19\n",
       "False positive (FP)      0         1.0      6\n",
       "True positive (TP)       1         1.0    123"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_to_drop = ['words', 'terms', 'hash', 'features', \n",
    "                'prediction', 'rawPrediction', 'probability']\n",
    "df_sms_test = df_sms_test.drop(*cols_to_drop)\n",
    "df_sms_train = df_sms_train.drop(*cols_to_drop)\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline_sms = Pipeline(stages=[tokenizer_sms, remover_sms, hasher_sms, idf_sms, logistic_sms])\n",
    "\n",
    "# Fitting the model on the trainning data\n",
    "pipeline_sms = pipeline_sms.fit(df_sms_train)\n",
    "df_sms_train = pipeline_sms.transform(df_sms_train)\n",
    "\n",
    "# Evaluating on the testing data\n",
    "predictions_sms = pipeline_sms.transform(df_sms_test)\n",
    "\n",
    "# predictions_sms.groupBy('label', 'prediction').count().show() # Confusion matrix\n",
    "cm_sms = predictions_sms.groupBy(\"label\", \"prediction\").count().toPandas().sort_values([\"prediction\", \"label\"])\n",
    "cm_sms.index = ['True negative (TN)', 'False negative (FN)',\n",
    "                         'False positive (FP)', 'True positive (TP)']\n",
    "TN = predictions_sms.filter('prediction = 0 AND label = 0').count()\n",
    "TP = predictions_sms.filter('prediction = 1 AND label = 1').count()\n",
    "FN = predictions_sms.filter('prediction = 0 AND label = 1').count()\n",
    "FP = predictions_sms.filter('prediction = 1 AND label = 0').count()\n",
    "\n",
    "# Accuracy measures the proportion of correct predictions\n",
    "accuracy = (TN + TP) / (TN + TP + FN + FP)\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "print(f'''\n",
    "Accuracy : {accuracy}\n",
    "Precision: {precision}\n",
    "Recall   : {recall}\n",
    "''')\n",
    "cm_sms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfb9691-7128-40a8-b2a4-9bfd28dfbce9",
   "metadata": {},
   "source": [
    "## Ex. 8 - SMS spam optimised\n",
    "\n",
    "The pipeline you built earlier for the SMS spam model used the default parameters for all of the elements in the pipeline. It's very unlikely that these parameters will give a particularly good model though. In this exercise you're going to run the pipeline for a selection of parameter values. We're going to do this in a systematic way: the values for each of the hyperparameters will be laid out on a grid and then pipeline will systematically run across each point in the grid.\n",
    "\n",
    "In this exercise you'll set up a parameter grid which can be used with cross validation to choose a good set of parameters for the SMS spam classifier.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Create a parameter grid builder object.\n",
    "2. Add grid points for `numFeatures` and `binary` parameters to the `HashingTF` object, giving values `1024`, `4096` and `16384`, and `True` and `False`, respectively.\n",
    "3. Add grid points for `regParam` and `elasticNetParam` parameters to the `LogisticRegression` object, giving values of `0.01`, `0.1`, `1.0` and `10.0`, and `0.0`, `0.5`, and `1.0` respectively.\n",
    "4. Build the parameter grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fb69a69a-5d76-4c53-b751-00189cad7404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models created:  72\n"
     ]
    }
   ],
   "source": [
    "# Create parameter grid\n",
    "params_sms = (ParamGridBuilder().addGrid(hasher_sms.numFeatures, [1024, 4096, 16384])    # Params for hashing\n",
    "                                .addGrid(hasher_sms.binary, [True, False])\n",
    "                                .addGrid(logistic_sms.regParam, [0.01, 0.1, 1.0, 10.0])  # Params for logReg\n",
    "                                .addGrid(logistic_sms.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "                                .build())\n",
    "print('Number of models created: ', len(params_sms))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3573b41a-4130-44db-8266-2b011c272c96",
   "metadata": {},
   "source": [
    "## Ex. 9 - How many models for grid search?\n",
    "\n",
    "How many models will be built in the cross-validator?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ab2d5145-6a5e-4ede-b9a8-68e467983bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models in the cross validator:  360\n"
     ]
    }
   ],
   "source": [
    "# The cross-validation object.\n",
    "numfolds = 5\n",
    "cv_flight = CrossValidator(estimator=pipeline_flight, estimatorParamMaps=params,\n",
    "                           evaluator=evaluator_flight, numFolds=numfolds, seed=SEED)\n",
    "print('Number of models in the cross validator: ', len(params_sms)*numfolds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1bfcef-d3b7-45e6-bee3-784106b2fa99",
   "metadata": {},
   "source": [
    "# Close session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8ebc4ec7-959d-469e-ae7c-7e706ea129ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
